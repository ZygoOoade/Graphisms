{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d367e54a42154e00a019facb26133459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bed708d13624cdf96cdaea1315d41c6",
              "IPY_MODEL_5f41754826bc4a65977ea90f96233c32",
              "IPY_MODEL_92b6c8f0b40a4837aac7cf6036281a1c"
            ],
            "layout": "IPY_MODEL_191b7867d67b47ef83fc0cb9d34710ca"
          }
        },
        "9bed708d13624cdf96cdaea1315d41c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d8e5f78a8514554bbe9850889f575a2",
            "placeholder": "​",
            "style": "IPY_MODEL_42fe1665f3754310bf336a790b9f1b27",
            "value": " 79%"
          }
        },
        "5f41754826bc4a65977ea90f96233c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5cb2b296fdd47a18b0fa2e3ce6c7b7d",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e0badc1e4984500a3a7de39706244b9",
            "value": 67
          }
        },
        "92b6c8f0b40a4837aac7cf6036281a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_478ba199c64c487b9f3b2362675023f7",
            "placeholder": "​",
            "style": "IPY_MODEL_774adb8ebba1452381887071de586d44",
            "value": " 67/85 [49:09&lt;13:17, 44.30s/it]"
          }
        },
        "191b7867d67b47ef83fc0cb9d34710ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8e5f78a8514554bbe9850889f575a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42fe1665f3754310bf336a790b9f1b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5cb2b296fdd47a18b0fa2e3ce6c7b7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e0badc1e4984500a3a7de39706244b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "478ba199c64c487b9f3b2362675023f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774adb8ebba1452381887071de586d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw68Q4iixpLz",
        "outputId": "320c0ca3-754f-41e5-cc77-1ffacb7d1e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into '/content/TotoroUI'...\n",
            "remote: Enumerating objects: 14652, done.\u001b[K\n",
            "remote: Counting objects: 100% (3401/3401), done.\u001b[K\n",
            "remote: Compressing objects: 100% (307/307), done.\u001b[K\n",
            "remote: Total 14652 (delta 3244), reused 3094 (delta 3094), pack-reused 11251 (from 1)\u001b[K\n",
            "Receiving objects: 100% (14652/14652), 22.10 MiB | 7.15 MiB/s, done.\n",
            "Resolving deltas: 100% (9939/9939), done.\n",
            "/content/TotoroUI\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hThe following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            " *** Download Progress Summary as of Fri Nov  1 17:31:39 2024 *** \n",
            "=\n",
            "[#e10c6b 10GiB/11GiB(96%) CN:16 DL:178MiB ETA:1s]\n",
            "FILE: /content/TotoroUI/models/unet/flux1-dev-fp8.safetensors\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "e10c6b|\u001b[1;32mOK\u001b[0m  |   175MiB/s|/content/TotoroUI/models/unet/flux1-dev-fp8.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "252f93|\u001b[1;32mOK\u001b[0m  |   255MiB/s|/content/TotoroUI/models/vae/ae.sft\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "e00c55|\u001b[1;32mOK\u001b[0m  |   210MiB/s|/content/TotoroUI/models/clip/clip_l.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "68be62|\u001b[1;32mOK\u001b[0m  |   151MiB/s|/content/TotoroUI/models/clip/t5xxl_fp8_e4m3fn.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "WARNING:root:clip missing: ['text_projection.weight']\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro import model_management\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    positive_prompt = \"a hand pointing against a black background\"\n",
        "    negative_prompt = \"blurry, low resolution, bad, ugly, low quality, pixelated, interpolated, compression artifacts, noisey, grainy, deformed finger, ugliness, sadness, artifact, poor quality, defect, nsfw, virility, facial deformity, deformed hand, deformation, deformities, malformations, sexuality, nudity\"\n",
        "    width = 1053\n",
        "    height = 1872\n",
        "    seed = 0\n",
        "    steps = 85\n",
        "    sampler_name = \"euler\"\n",
        "    scheduler = \"simple\"\n",
        "\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(seed)\n",
        "\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 1.0)[0]\n",
        "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(\"/content/img.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "d367e54a42154e00a019facb26133459",
            "9bed708d13624cdf96cdaea1315d41c6",
            "5f41754826bc4a65977ea90f96233c32",
            "92b6c8f0b40a4837aac7cf6036281a1c",
            "191b7867d67b47ef83fc0cb9d34710ca",
            "3d8e5f78a8514554bbe9850889f575a2",
            "42fe1665f3754310bf336a790b9f1b27",
            "a5cb2b296fdd47a18b0fa2e3ce6c7b7d",
            "3e0badc1e4984500a3a7de39706244b9",
            "478ba199c64c487b9f3b2362675023f7",
            "774adb8ebba1452381887071de586d44"
          ]
        },
        "id": "eYoaL1AGxuWl",
        "outputId": "b7c19832-11d8-4da1-ab1d-005b72175df2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5346422519195259203\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d367e54a42154e00a019facb26133459",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/85 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "def decode_with_memory_management(sample, vae, tile_size=32, overlap=8, target_width=1053, target_height=1872):\n",
        "    if isinstance(sample, dict):\n",
        "        image_tensor = sample['samples']\n",
        "    else:\n",
        "        image_tensor = sample\n",
        "\n",
        "    # Get dimensions and pad to multiples of 8\n",
        "    batch_size, channels, height, width = image_tensor.shape\n",
        "    padded_width = math.ceil(width / 8) * 8\n",
        "    padded_height = math.ceil(height / 8) * 8\n",
        "\n",
        "    # Calculate output dimensions (VAE upscales by 8)\n",
        "    out_width = target_width\n",
        "    out_height = target_height\n",
        "\n",
        "    # Initialize the final numpy array and weight accumulator\n",
        "    decoded_numpy = np.zeros((3, out_height, out_width), dtype=np.float32)\n",
        "    weight_acc = np.zeros((out_height, out_width), dtype=np.float32)\n",
        "\n",
        "    # Calculate effective stride (tile_size - overlap)\n",
        "    stride = tile_size - overlap\n",
        "\n",
        "    # Calculate number of tiles\n",
        "    num_tiles_h = math.ceil((height - overlap) / stride)\n",
        "    num_tiles_w = math.ceil((width - overlap) / stride)\n",
        "\n",
        "    def create_gaussian_weight(size):\n",
        "        # Creates a Gaussian weight matrix for blending\n",
        "        x = np.linspace(-1, 1, size)\n",
        "        y = np.linspace(-1, 1, size)\n",
        "        xv, yv = np.meshgrid(x, y)\n",
        "        gaussian = np.exp(-((xv**2 + yv**2) / 0.5**2))\n",
        "        return gaussian.astype(np.float32)\n",
        "\n",
        "    try:\n",
        "        for h in range(num_tiles_h):\n",
        "            for w in range(num_tiles_w):\n",
        "                # Calculate tile boundaries with overlap\n",
        "                h_start = h * stride\n",
        "                w_start = w * stride\n",
        "                h_end = min(h_start + tile_size, height)\n",
        "                w_end = min(w_start + tile_size, width)\n",
        "\n",
        "                # Determine padding size, ensuring it doesn’t exceed the available dimensions\n",
        "                pad_right = max(0, min(tile_size - (w_end - w_start), width - w_start - (w_end - w_start)))\n",
        "                pad_bottom = max(0, min(tile_size - (h_end - h_start), height - h_start - (h_end - h_start)))\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # Extract tile and only pad if within acceptable limits\n",
        "                    tile = image_tensor[:, :, h_start:h_end, w_start:w_end].clone()\n",
        "                    if pad_right > 0 or pad_bottom > 0:\n",
        "                        if tile.shape[3] + pad_right <= tile_size and tile.shape[2] + pad_bottom <= tile_size:\n",
        "                            tile = F.pad(tile, (0, pad_right, 0, pad_bottom), mode='reflect')\n",
        "                        else:\n",
        "                            print(\"Skipping padding due to size limits at boundaries\")\n",
        "\n",
        "                    # Decode the tile\n",
        "                    decoded_tile = vae.decode(tile)\n",
        "                    tile_np = decoded_tile.cpu().numpy()\n",
        "\n",
        "                    # Ensure tile_np dimensions match expectations\n",
        "                    if tile_np.shape[-1] == 3:\n",
        "                        tile_np = np.transpose(tile_np, (0, 3, 1, 2))\n",
        "                    if tile_np.ndim == 4:\n",
        "                        tile_np = tile_np[0]\n",
        "\n",
        "                    # Calculate output positions\n",
        "                    out_h_start = h_start * 8\n",
        "                    out_w_start = w_start * 8\n",
        "                    out_h_end = min(out_h_start + tile_np.shape[1], out_height)\n",
        "                    out_w_end = min(out_w_start + tile_np.shape[2], out_width)\n",
        "\n",
        "                    # Adjust weight section dimensions to match tile dimensions\n",
        "                    tile_height = out_h_end - out_h_start\n",
        "                    tile_width = out_w_end - out_w_start\n",
        "                    weight_section = create_gaussian_weight(tile_size * 8)[:tile_height, :tile_width]\n",
        "\n",
        "                    # Verify that tile_np and weight_section have matching dimensions\n",
        "                    if tile_np.shape[1:3] != weight_section.shape:\n",
        "                        tile_np = tile_np[:, :tile_height, :tile_width]\n",
        "\n",
        "                    # Apply weighted accumulation\n",
        "                    for c in range(3):\n",
        "                        decoded_numpy[c, out_h_start:out_h_end, out_w_start:out_w_end] += \\\n",
        "                            tile_np[c, :tile_height, :tile_width] * weight_section\n",
        "                    weight_acc[out_h_start:out_h_end, out_w_start:out_w_end] += weight_section\n",
        "\n",
        "                torch.cuda.empty_cache()  # Free GPU memory\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during tiled processing: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Normalize by accumulated weights\n",
        "    mask = weight_acc > 0\n",
        "    for c in range(3):\n",
        "        decoded_numpy[c, mask] /= weight_acc[mask]\n",
        "\n",
        "    # Convert to uint8\n",
        "    decoded_numpy = (decoded_numpy * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "    return decoded_numpy\n",
        "\n",
        "def process_and_save_image_managed(sample, vae, output_path=\"/content/img.png\"):\n",
        "    try:\n",
        "        # Ensure we start with clean memory\n",
        "        decoded_numpy = decode_with_memory_management(sample, vae, target_width=1053, target_height=1872)\n",
        "        if decoded_numpy is not None:\n",
        "            # Transpose for PIL image format\n",
        "            final_image = np.transpose(decoded_numpy, (1, 2, 0))\n",
        "            Image.fromarray(final_image).save(output_path)\n",
        "\n",
        "            # Verify save by displaying the saved image\n",
        "            saved_image = Image.open(output_path)\n",
        "            display(saved_image)\n",
        "            print(\"Image successfully processed, saved, and displayed.\")\n",
        "            return final_image\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(f\"Error details: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "with torch.no_grad():\n",
        "    result = process_and_save_image_managed(\n",
        "        sample, vae, output_path=\"/content/img.png\"\n",
        "    )\n",
        "    if result is not None:\n",
        "        print(\"Success\")"
      ],
      "metadata": {
        "id": "JgutagMw9kNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}